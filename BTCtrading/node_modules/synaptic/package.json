{
  "name": "synaptic",
  "version": "0.1.1",
  "description": "Architecture-free neural network library",
  "main": "./lib/synaptic",
  "scripts": {
    "test": "mocha test"
  },
  "devDependencies": {
    "mocha": ">=1.0.0"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/cazala/synaptic.git"
  },
  "keywords": [
    "neural network",
    "machine learning",
    "long short term memory",
    "perceptron",
    "architecture free"
  ],
  "author": {
    "name": "Juan Cazala",
    "email": "juancazala@gmail.com",
    "url": "http://juancazala.com/"
  },
  "license": {
    "type": "MIT",
    "url": "https://github.com/cazala/synaptic/blob/master/LICENSE"
  },
  "bugs": {
    "url": "https://github.com/cazala/synaptic/issues"
  },
  "homepage": "http://synaptic.juancazala.com",
  "readme": "Synaptic\n========\n\nSynaptic is a javascript neural network library for **node.js** and the **browser**, its generalized algorythm is architecture-free, so you can build and train basically any type of first order or even [second order neural network](http://en.wikipedia.org/wiki/Recurrent_neural_network#Second_Order_Recurrent_Neural_Network) architectures.\n\nThis library includes a few built-in architectures like [multilayer perceptrons](http://en.wikipedia.org/wiki/Multilayer_perceptron), [multilayer long-short term memory](http://en.wikipedia.org/wiki/Long_short_term_memory) networks (LSTM) or [liquid state machines](http://en.wikipedia.org/wiki/Liquid_state_machine), and a trainer capable of training any given network, which includes built-in training tasks/tests like solving an XOR, completing a Distracted Sequence Recall task or an [Embeded Reber Grammar](http://www.willamette.edu/~gorr/classes/cs449/reber.html) test, so you can easily test and compare the performance of different architectures.\n\n\nThe algorythm implemented by this library has been taken from Derek D. Monner's paper:\n\n[A generalized LSTM-like training algorithm for second-order recurrent neural networks](http://www.overcomplete.net/papers/nn2012.pdf)\n\n\nThere are references to the equations in that paper commented through the source code.\n\n##Overview\n\n###Installation\n\n######In node\nYou can install synaptic with [npm](http://npmjs.org):\n\n`npm install synaptic --save`\n\n######In the browser\nJust include the file synaptic.js (you can find it in the `/lib` directory) with a script tag in your HTML:\n\n`<script src=\"synaptic.js\"></script>`\n\n###Usage\n\n```\nvar synaptic = require('synaptic'); // this line is not needed in the browser\nvar Neuron = synaptic.Neuron,\n\tLayer = synaptic.Layer,\n\tNetwork = synaptic.Network,\n\tTrainer = synaptic.Trainer,\n\tArchitect = synaptic.Architect;\n```\n\nNow you can start to create networks, train them, or use built-in networks from the [Architect](http://github.com/cazala/synaptic#architect).\n\n###Demos\n\n- [Solve an XOR](http://synaptic.juancazala.com/xor.html)\n- [Discrete Sequence Recall Task](http://synaptic.juancazala.com/dsr.html)\n- [Learn Image Filters](http://synaptic.juancazala.com/filter.html)\n- [Paint an Image](http://synaptic.juancazala.com/paint.html)\n- [Read from Wikipedia](http://synaptic.juancazala.com/wiki.html)\n\n###Examples\n\n######Perceptron\n\nThis is how you can create a simple [perceptron](http://www.codeproject.com/KB/dotnet/predictor/network.jpg).\n\n```\nfunction Perceptron(input, hidden, output)\n{\n\t// create the layers\n\tvar inputLayer = new Layer(input);\n\tvar hiddenLayer = new Layer(hidden);\n\tvar outputLayer = new Layer(output);\n\n\t// connect the layers\n\tinputLayer.project(hiddenLayer);\n\thiddenLayer.project(outputLayer);\n\n\t// set the layers\n\tthis.set({\n\t\tinput: inputLayer,\n\t\thidden: [hiddenLayer],\n\t\toutput: outputLayer\n\t});\n}\n\n// extend the prototype chain\nPerceptron.prototype = new Network();\nPerceptron.prototype.constructor = Perceptron;\n```\n\nNow you can test your new network by creating a trainer and teaching the perceptron to learn an XOR\n\n```\nvar myPerceptron = new Perceptron(2,3,1);\nvar myTrainer = new Trainer(myPerceptron);\n\nmyTrainer.XOR(); // { error: 0.004998819355993572, iterations: 21871, time: 356 }\n\nmyPerceptron.activate([0,0]); // 0.0268581547421616\nmyPerceptron.activate([1,0]); // 0.9829673642853368\nmyPerceptron.activate([0,1]); // 0.9831714267395621\nmyPerceptron.activate([1,1]); // 0.02128894618097928\n```\n\n######Long Short-Term Memory\n\nThis is how you can create a simple [long short-term memory](http://people.idsia.ch/~juergen/lstmcell4.jpg) with input gate, forget gate, output gate, and peephole connections.\n\n```\nfunction LSTM(input, blocks, output)\n{\n\t// create the layers\n\tvar inputLayer = new Layer(input);\n\tvar inputGate = new Layer(blocks);\n\tvar forgetGate = new Layer(blocks);\n\tvar memoryCell = new Layer(blocks);\n\tvar outputGate = new Layer(blocks);\n\tvar outputLayer = new Layer(output);\n\n\t// connections from input layer\n\tvar input = inputLayer.project(memoryCell);\n\tinputLayer.project(inputGate);\n\tinputLayer.project(forgetGate);\n\tinputLayer.project(outputGate);\n\n\t// connections from memory cell\n\tvar output = memoryCell.project(outputLayer);\n\n\t// self-connection\n\tvar self = memoryCell.project(memoryCell, Layer.connectionType.ONE_TO_ONE);\n\n\t// peepholes\n\tmemoryCell.project(inputGate,  Layer.connectionType.ONE_TO_ONE);\n\tmemoryCell.project(forgetGate, Layer.connectionType.ONE_TO_ONE);\n\tmemoryCell.project(outputGate, Layer.connectionType.ONE_TO_ONE);\n\n\t// gates\n\tinputGate.gate(input, Layer.gateType.INPUT);\n\tforgetGate.gate(self, Layer.gateType.ONE_TO_ONE);\n\toutputGate.gate(output, Layer.gateType.OUTPUT);\n\n\t// input to output direct connection\n\tinputLayer.project(outputLayer);\n\n\t// set the layers of the neural network\n\tthis.set({\n\t\tinput: inputLayer,\n\t\thidden: hiddenLayers,\n\t\toutput: outputLayer\n\t});\n}\n\n// extend the prototype chain\nLSTM.prototype = new Network();\nLSTM.prototype.constructor = LSTM;\n```\n\nThese are examples for explanatory purposes, the [Architect](http://github.com/cazala/synaptic#architect) already includes Multilayer Perceptrons and\nMultilayer LSTM networks architectures.\n\n\nDocumentation\n=============\n\n##Neuron\n\nNeurons are the basic unit of the neural network. They can be connected together, or used to gate connetions between other neurons. \nA Neuron can perform basically 4 operations: project connections, gate connections, activate and propagate.\n\n######project\n\nA neuron can project a connection to another neuron (i.e. connect neuron A with neuron B).\nHere is how it's done:\n\n```\nvar A = new Neuron();\nvar B = new Neuron();\nA.project(B); // A now projects a connection to B\n```\n\nNeurons can also self-connect:\n\n`A.project(A);`\n\nThe method **project** returns a `Connection` object, that can be gated by another neuron.\n\n######gate\n\nA neuron can gate a connection between two neurons, or a neuron's self-connection. This allows you to create [second order neural network](http://en.wikipedia.org/wiki/Recurrent_neural_network#Second_Order_Recurrent_Neural_Network) architectures.\n\n```\nvar A = new Neuron();\nvar B = new Neuron();\nvar connection = A.project(B);\n\nvar C = new Neuron();\nC.gate(connection); // now C gates the connection between A and B\n```\n\n\n######activate\n\nWhen a neuron activates, it computes it's state from all its input connections and squashes it using its activation function, and returns the output (activation).\nYou can provide the activation as a parameter (useful for neurons in the input layer. it has to be a float between 0 and 1). For example:\n\n```\nvar A = new Neuron();\nvar B = new Neuron();\nA.project(B);\n\nA.activate(0.5); // 0.5\nB.activate(); // 0.3244554645\n```\n\n\n######propagate\n\nAfter an activation, you can teach the neuron what should have been the correct output (a.k.a. train). This is done by backpropagating the error.\nTo use the **propagate** method you have to provide a learning rate, and a target value (float between 0 and 1).\n\nFor example, if I want to train neuron B to output a value close to 0 when neuron A activates a value of 1, with an error smaller than 0.005:\n\n```\nvar A = new Neuron();\nvar B = new Neuron();\nA.project(B);\n\nvar learningRate = .3,\n\ttargetOutput = 0,\n\terror = 0.005,\n\toutput = 1;\n\nwhile(output - targetOutput > error)\n{\n\tA.activate(1);\n\n\toutput = B.activate();\n\tB.propagate(learningRate, targetOutput);\n}\n\n// test it\nA.activate(1);\nB.activate(); // 0.0049998578298219975\n```\n\n######squashing function and bias\n\nBy default, a neuron uses a [Logistic Sigmoid](http://en.wikipedia.org/wiki/Logistic_function) as its squashing/activation function, and a random bias. \nYou can change those properties the following way:\n\n```\nvar A = new Neuron();\nA.squash = Neuron.squash.TANH;\nA.bias = 1;\n```\n\nThere are 4 built-in squashing functions, but you can also create your own:\n\n- [Neuron.squash.LOGISTIC](http://commons.wikimedia.org/wiki/File:SigmoidFunction.png)\n- [Neuron.squash.TANH](http://commons.wikimedia.org/wiki/File:TanhFunction.jpg)\n- [Neuron.squash.IDENTITY](http://en.wikipedia.org/wiki/File:Function-x.svg)\n- [Neuron.squash.HLIM](http://commons.wikimedia.org/wiki/File:HardLimitFunction.png)\n\n##Layer\n\nNormally you won't work with single neurons, but use Layers instead. A layer is basically an array of neurons, they can do pretty much the same things as neurons do, but it makes the programming process faster.\n\nTo create a layer you just have to specify its size (the number of neurons in that layer).\n\n`var myLayer = new Layer(5);`\n\n######project\n\nA layer can project a connection to another layer. You have to provide the layer that you want to connect to and the `connectionType`:\n\n```\nvar A = new Layer(5);\nvar B = new Layer(3);\nA.project(B, Layer.connectionType.ALL_TO_ALL); // All the neurons in layer A now project a connection to all the neurons in layer B\n```\n\nLayers can also self-connect:\n\n`A.project(A, Layer.connectionType.ONE_TO_ONE);`\n\nThere are two `connectionType`'s:\n- Layer.connectionType.ALL_TO_ALL\n- Layer.connectionType.ONE_TO_ONE\n\nIf not specified, the connection type is always `Layer.connectionType.ALL_TO_ALL.`If you make a one-to-one connection, both layers must have **the same size**.\n\nThe method **project** returns a `LayerConnection` object, that can be gated by another layer.\n\n######gate\n\nA layer can gate a connection between two other layers, or a layers's self-connection.\n\n```\nvar A = new Layer(5);\nvar B = new Layer(3);\nvar connection = A.project(B);\n\nvar C = new Layer(4);\nC.gate(connection, Layer.gateType.INPUT_GATE); // now C gates the connection between A and B (input gate)\n```\n\nThere are three `gateType`'s:\n- Layer.gateType.INPUT_GATE: If layer C is gating connections between layer A and B, all the neurons from C gate all the input connections to B.\n- Layer.gateType.OUTPUT_GATE: If layer C is gating connections between layer A and B, all the neurons from C gate all the output connections from A.\n- Layer.gateType.ONE_TO_ONE: If layer C is gating connections between layer A and B, each neuron from C gates one connection from A to B. This is useful for gating self-connected layers. To use this kind of gateType, A, B and C must be the same size.\n\n######activate\n\nWhen a layer activates, it just activates all the neurons in that layer in order, and returns an array with the outputs. It accepts an array of activations as parameter (for input layers):\n\n```\nvar A = new Layer(5);\nvar B = new Layer(3);\nA.project(B);\n\nA.activate([1,0,1,0,1]); // [1,0,1,0,1]\nB.activate(); // [0.3280457, 0.83243247, 0.5320423]\n```\n\n######propagate\n\nAfter an activation, you can teach the layer what should have been the correct output (a.k.a. train). This is done by backpropagating the error.\nTo use the **propagate** method you have to provide a learning rate, and a target value (array of floats between 0 and 1).\n\nFor example, if I want to train layer B to output [0,0] when layer A activates [1,0,1,0,1], with an error smaller than 0.005:\n\n```\nvar A = new Layer(5);\nvar B = new Layer(2);\nA.project(B);\n\nvar learningRate = .3,\n\ttargetOutput = [0,0],\n\terror = 0.005\n\toutput = [1,1];\n\nwhile(output[0] - targetOutput[0] > error && output[1] - targetOutput[1] > error)\n{\n\tA.activate([1,0,1,0,1]);\n\n\toutput = B.activate();\n\tB.propagate(learningRate, targetOutput);\n}\n\n// test it\nA.activate([1,0,1,0,1]);\nB.activate(); // [0.004999850993267468, 0.00499980138183861]\n```\n\n######squashing function and bias\n\nYou can set the squashing function and bias of all the neurons in a layer by using the method **set**:\n\n```\nmyLayer.set({\n\tsquash: Neuron.squash.TANH,\n\tbias: 0\n})\n```\n\n######neurons\n\nThe method `neurons()` return an array with all the neurons in the layer, in activation order.\n\n##Network\n\nNetworks are basically an array of layers. They have an input layer, a number of hidden layers, and an output layer. Networks can project and gate connections, activate and propagate in the same fashion as [Layers](http://github.com/cazala/synaptic#layer) do. Networks can also be optimized, extended, exported to JSON, converted to Workers or standalone Functions, and cloned.\n\n```\nvar inputLayer = new Layer(4);\nvar hiddenLayer = new Layer(6);\nvar outputLayer = new Layer(2);\n\ninputLayer.project(hiddenLayer);\nhiddenLayer.project(outputLayer);\n\nvar myNetwork = new Network({\n\tinput: inputLayer,\n\thidden: [hiddenLayer],\n\toutput: outputLayer\n});\n```\n######project\n\nA network can project a connection to another, or gate a connection between two others networks in the same way [Layers](http://github.com/cazala/synaptic#layer) do.\nYou have to provide the network that you want to connect to and the `connectionType`:\n\n```\nmyNetwork.project(otherNetwork, Layer.connectionType.ALL_TO_ALL); \n/* \t\n\tAll the neurons in myNetwork's output layer now project a connection\n\tto all the neurons in otherNetowrk's input layer.\n*/\n```\n\nThere are two `connectionType`'s:\n- Layer.connectionType.ALL_TO_ALL\n- Layer.connectionType.ONE_TO_ONE\n\nIf not specified, the connection type is always `Layer.connectionType.ALL_TO_ALL.`If you make a one-to-one connection, both layers must have **the same size**.\n\nThe method **project** returns a `LayerConnection` object, that can be gated by another network or layer.\n\n######gate\n\nA Network can gate a connection between two other Networks or Layers, or a Layers's self-connection.\n\n```\nvar connection = A.project(B);\nC.gate(connection, Layer.gateType.INPUT_GATE); // now C's output layer gates the connection between A's output layer and B's input layer (input gate)\n```\n\nThere are three `gateType`'s:\n- Layer.gateType.INPUT_GATE: If netowork C is gating connections between network A and B, all the neurons from C's output layer gate all the input connections to B's input layer.\n\n- Layer.gateType.OUTPUT_GATE: If network C is gating connections between network A and B, all the neurons from C's output layer gate all the output connections from A's output layer.\n\n- Layer.gateType.ONE_TO_ONE: If network C is gating connections between network A and B, each neuron from C's output layer gates one connection from A's output layer to B's input layer. To use this kind of gateType, A's output layer, B's input layer and C's output layer must be the same size.\n\n######activate\n\nWhen a network is activeted, an input must be provided to activate the input layer, then all the hidden layers are activated in order, and finally the output layer is activated and its activation is returned.\n\n```\nvar inputLayer = new Layer(4);\nvar hiddenLayer = new Layer(6);\nvar outputLayer = new Layer(2);\n\ninputLayer.project(hiddenLayer);\nhiddenLayer.project(outputLayer);\n\nvar myNetwork = new Network({\n\tinput: inputLayer,\n\thidden: [hiddenLayer],\n\toutput: outputLayer\n});\n\nmyNetwork.activate([1,0,1,0]); // [0.5200553602396137, 0.4792707231811006]\n```\n\n######propagate\n\nYou can provide a target value and a learning rate to a network and backpropagate the error from the output layer to all the hidden layers in reverse order until reaching the input layer. For example, this is how you train a network how to solve an XOR:\n\n```\n// create the network\nvar inputLayer = new Layer(2);\nvar hiddenLayer = new Layer(3);\nvar outputLayer = new Layer(1);\n\ninputLayer.project(hiddenLayer);\nhiddenLayer.project(outputLayer);\n\nvar myNetwork = new Network({\n\tinput: inputLayer,\n\thidden: [hiddenLayer],\n\toutput: outputLayer\n});\n\n// train the network\nvar learningRate = .3;\nfor (var i = 0; i < 20000; i++)\n{\n\t// 0,0 => 0\n\tmyNetwork.activate([0,0]);\n\tmyNetwork.propagate(learningRate, [0]);\n\n\t// 0,1 => 1\n\tmyNetwork.activate([0,1]);\n\tmyNetwork.propagate(learningRate, [1]);\n\n\t// 1,0 => 1\n\tmyNetwork.activate([1,0]);\n\tmyNetwork.propagate(learningRate, [1]);\n\n\t// 1,1 => 0\n\tmyNetwork.activate([1,1]);\n\tmyNetwork.propagate(learningRate, [0]);\n}\n\n\n// test the network\nmyNetwork.activate([0,0]); // [0.015020775950893527]\nmyNetwork.activate([0,1]); // [0.9815816381088985]\nmyNetwork.activate([1,0]); // [0.9871822457132193]\nmyNetwork.activate([1,1]); // [0.012950087641929467]\n```\n\n######optimize\n\nNetworks get optimized automatically on the fly after its first activation, if you print in the console the `activate` or `propagate` methods of your Network instance after activating it, it will look something like this:\n\n```\nfunction (input){\nF[1] = input[0];\n F[2] = input[1];\n F[3] = input[2];\n F[4] = input[3];\n F[6] = F[7];\n F[7] = F[8];\n F[7] += F[1] * F[9];\n F[7] += F[2] * F[10];\n F[7] += F[3] * F[11];\n F[7] += F[4] * F[12];\n F[5] = (1 / (1 + Math.exp(-F[7])));\n F[13] = F[5] * (1 - F[5]);\n ...\n ```\n\nThis improves the performance of the network dramatically.\n\n######extend\n\nYou can see how to extend a network in the [Examples](http://github.com/cazala/synaptic#examples) section.\n\n######toJSON/fromJSON\n\nNetworks can be stored as JSON's and then restored back:\n\n```\nvar exported = myNetwork.toJSON();\nvar imported = Network.fromJSON(exported);\n```\n\n######worker\n\nThe network can be converted into a WebWorker. This feature doesn't work in node.js, and it's not supported on every browser (it must support Blob).\n\n```\n// training set\nvar learningRate = .3;\nvar trainingSet = [\n\t{\n\t\tinput: [0,0],\n\t\toutput: [0]\n\t},\n\t{\n\t\tinput: [0,1],\n\t\toutput: [1]\n\t},\n\t{\n\t\tinput: [1,0],\n\t\toutput: [1]\n\t},\n\t{\n\t\tinput: [1,1],\n\t\toutput: [0]\n\t},\n];\n\n// create a network\nvar inputLayer = new Layer(2);\nvar hiddenLayer = new Layer(3);\nvar outputLayer = new Layer(1);\n\ninputLayer.project(hiddenLayer);\nhiddenLayer.project(outputLayer);\n\nvar myNetwork = new Network({\n\tinput: inputLayer,\n\thidden: [hiddenLayer],\n\toutput: outputLayer\n});\n\n// create a worker\nvar myWorker = myNetwork.worker();\n\n// activate the network\nfunction activateWorker(input)\n{\n\tmyWorker.postMessage({ \n\t\taction: \"activate\",\n\t\tinput: input,\n\t\tmemoryBuffer: myNetwork.optimized.memory\n\t}, [myNetwork.optimized.memory.buffer]);\n}\n\n// backpropagate the network\nfunction propagateWorker(target){\n\tmyWorker.postMessage({ \n\t\taction: \"propagate\",\n\t\ttarget: target,\n\t\trate: learningRate,\n\t\tmemoryBuffer: myNetwork.optimized.memory\n\t}, [myNetwork.optimized.memory.buffer]);\n}\n\n// train the worker\nmyWorker.onmessage = function(e){\n\t// give control of the memory back to the network - this is mandatory!\n\tmyNetwork.optimized.ownership(e.data.memoryBuffer);\n\n\tif (e.data.action == \"propagate\")\n\t{\n\t\tif (index >= 4)\n\t\t{\n\t\t\tindex = 0;\n\t\t\titerations++;\n\t\t\tif (iterations % 100 == 0)\n\t\t\t{\n\t\t\t\tvar output00 = myNetwork.activate([0,0]);\n\t\t\t\tvar output01 = myNetwork.activate([0,1]);\n\t\t\t\tvar output10 = myNetwork.activate([1,0]);\n\t\t\t\tvar output11 = myNetwork.activate([1,1]);\n\n\t\t\t\tconsole.log(\"0,0 => \", output00);\n\t\t\t\tconsole.log(\"0,1 => \", output01);\n\t\t\t\tconsole.log(\"1,0 => \", output10);\n\t\t\t\tconsole.log(\"1,1 => \", output11, \"\\n\");\n\t\t\t}\n\t\t}\n\n\t\tactivateWorker(trainingSet[index].input);\n\t}\n\n\tif (e.data.action == \"activate\")\n\t{\n\t\tpropagateWorker(trainingSet[index].output);\t\n\t\tindex++;\n\t}\n}\n\n// kick it\nvar index = 0;\nvar iterations = 0;\nactivateWorker(trainingSet[index].input);\n```\n\n######standalone\n\nThe network can be exported to a single javascript Function. This can be useful when your network is already trained and you just need to use it, since the standalone functions is just one javascript function with an array and operations within, with no dependencies on Synaptic or any other library.\n\n```\nvar inputLayer = new Layer(4);\nvar hiddenLayer = new Layer(6);\nvar outputLayer = new Layer(2);\n\ninputLayer.project(hiddenLayer);\nhiddenLayer.project(outputLayer);\n\nvar myNetwork = new Network({\n\tinput: inputLayer,\n\thidden: [hiddenLayer],\n\toutput: outputLayer\n});\n\nvar standalone = myNetwork.standalone();\n\nmyNetwork.activate([1,0,1,0]); \t// [0.5466397925108878, 0.5121246668637663]\nstandalone([1,0,1,0]);\t // [0.5466397925108878, 0.5121246668637663]\n```\n\n######clone\n\nA network can be cloned to a completely new instance, with the same connections and traces.\n\n```\nvar inputLayer = new Layer(4);\nvar hiddenLayer = new Layer(6);\nvar outputLayer = new Layer(2);\n\ninputLayer.project(hiddenLayer);\nhiddenLayer.project(outputLayer);\n\nvar myNetwork = new Network({\n\tinput: inputLayer,\n\thidden: [hiddenLayer],\n\toutput: outputLayer\n});\n\nvar clone = myNetwork.clone();\n\nmyNetwork.activate([1,0,1,0]); \t// [0.5466397925108878, 0.5121246668637663]\nclone.activate([1,0,1,0]);\t // [0.5466397925108878, 0.5121246668637663]\n```\n\n######neurons\n\nThe method `neurons()` return an array with all the neurons in the network, in activation order.\n\n######set\n\nThe method `set(layers)` receives an object with layers in the same format as the constructor of `Network` and sets them as the layers of the Network, this is useful when you are extending the `Network` class to create your own architectures. See the [examples](http://github.com/cazala/synaptic#examples) section.\n\n```\nvar inputLayer = new Layer(4);\nvar hiddenLayer = new Layer(6);\nvar outputLayer = new Layer(2);\n\ninputLayer.project(hiddenLayer);\nhiddenLayer.project(outputLayer);\n\nvar myNetwork = new Network();\n\nmyNetwork.set({\n\tinput: inputLayer,\n\thidden: [hiddenLayer],\n\toutput: outputLayer\n});\n```\n\n##Trainer\n\nThe `Trainer` makes it easier to train any set to any network, no matter its architecture. To create a trainer you just have to provide a `Network` to train.\n\n`var trainer = new Trainer(myNetwork);`\n\nThe trainer also contains bult-in tasks to test the performance of your network.\n\n######train\n\nThis method allows you to train any training set to a `Network`, the training set must be an `Array` containing object with an **input** and **output** properties, for exmple, this is how you train an XOR to a network using a trainer:\n\n```\nvar trainingSet = [\n\t{\n\t\tinput: [0,0],\n\t\toutput: [0]\n\t},\n\t{\n\t\tinput: [0,1],\n\t\toutput: [1]\n\t},\n\t{\n\t\tinput: [1,0],\n\t\toutput: [1]\n\t},\n\t{\n\t\tinput: [1,1],\n\t\toutput: [0]\n\t},\n];\n\nvar trainer = new Trainer(myNetwork);\ntrainer.train(trainingSet);\n```\nYou can also set different options for the training in an object as a second parameter, like:\n\ntrainer.train(trainingSet,{\n\trate: .1,\n\titerations: 20000,\n\terror: .005,\n\tshuffle: true,\n\tlog: 1000\n});\n\n- **rate**: learning rate to train the network.\n- **iterations**: maximum number of iterations\n- **error**: minimum error\n- **shuffle**: if true, the training set is shuffled after every iteration, this is useful for training data sequences which order is not meaningful to networks with context memory, like LSTM's.\n- **log**: this commands the trainer to console.log the error and iterations every X number of iterations.\n- **customLog**: you can create custom logs like this one:\n\n```\ncustomLog: {\n\tevery: 500,\n\tdo: function(error, iterations) {\n\t\tconsole.log(\"error\", error, \"iterations\", iterations);\n\t}\n}\n```\n\nWhen the training is done this method returns an object with the error, the iterations, and the elapsed time of the training.\n\n######XOR\n\nThis method trains an XOR to the network, is useful when you are experimenting with different architectures and you want to test and compare their performances:\n\n```\nvar trainer = new Trainer(myNetwork);\ntrainer.XOR(); // {error: 0.004999821588193305, iterations: 21333, time: 111}\n```\n\n######DSR\n\nThis method trains the network to complete a [Discrete Sequence Recall](http://synaptic.juancazala.com/dsr.html), which is a task for testing context memory in neural networks.\n\n```\ntrainer.DSR({\n\ttargets: [2,4],\n\tdistractors: [3,5],\n\tprompts: [0,1],\t\n\tlength: 10\t\n});\n```\n\n\n######ERG\n\nThis method trains the network to pass an [Embeded Reber Grammar](http://www.willamette.edu/~gorr/classes/cs449/reber.html) test.\n\n`trainer.ERG();`\n\n\n##Architect\n\nThe Architect contains built-in architectures, ready to use.\n\n######Perceptron\n\nThis architecture allows you to create multilayer perceptrons, also known as feed-forward neural networks. They consists on a sequence of layers, each fully connected to the next one. \n\n![multilayer perceptron](http://www.codeproject.com/KB/dotnet/predictor/network.jpg \"Multilayer Perceptron Architecture\")\n\nYou have to provide a minimum of 3 layers (input, hidden and output), but you can use as many hidden layers as you wish. This is a `Perceptron` with 2 neurons in the input layer, 3 neurons in the hidden layer, and 1 neuron in the output layer:\n\n`var myPerceptron = new Architect.Perceptron(2,3,1);`\n\nAnd this is a deep multilayer perceptron with 2 neurons in the input layer, 4 hidden layers with 10 neurons each, and 1 neuron in the output layer\n\n`var myPerceptron = new Architect.Perceptron(2, 10, 10, 10, 10, 1);`\n\n######LSTM\n\nThe [long short-term memory](http://en.wikipedia.org/wiki/Long_short_term_memory) is an architecture well-suited to learn from experience to classify, process and predict time series when there are very long time lags of unknown size between important events.\n\n![long short-term memory](http://people.idsia.ch/~juergen/lstmcell4.jpg \"Long Short-Term Memory Architecture\")\n\nTo use this architecture you have to set at least one input layer, one memory block assembly (consisting of four layers: input gate, memory cell, forget gate and output gate), and an output layer.\n\n`var myLSTM = new Architect.LSTM(2,6,1);` \n\nAlso you can set many layers of memory blocks:\n\n`var myLSTM = new Architect.LSTM(2,4,4,4,1);` \n\nThat LSTM network has three memory block assemblies, with 4 memory blocks each, and their own input gates, memory cells, forget gates and output gates.\n\n######Liquid\n\nThe `Liquid` architecture allows you to create [Liquid State Machines](http://en.wikipedia.org/wiki/Liquid_state_machine). In these networks, neurons are randomly connected to each other. The recurrent nature of the connections turns the time varying input into a spatio-temporal pattern of activations in the network nodes.\n\nTo use this architecture you have to set the size of the input layer, the size of the pool, the size of the output layer, the number of random connections in the pool, and the number of random gates among the connections.\n\n```\nvar input = 2;\nvar pool = 20;\nvar output = 1;\nvar connections = 30;\nvar gates = 10;\n\nvar myLiquidStateMachine = new Architect.Liquid(input, pool, output, connections, gates);\n```\n\nYou can create your own architectures by extending the `Network` class. You can check the [Examples](http://github.com/cazala/synaptic#examples) section for more information about this.\n\n\n",
  "readmeFilename": "README.md",
  "_id": "synaptic@0.1.1",
  "dist": {
    "shasum": "38629485c1c02a78fdefafb5bf30210cbcbd6e9e"
  },
  "_from": "synaptic@",
  "_resolved": "https://registry.npmjs.org/synaptic/-/synaptic-0.1.1.tgz"
}
